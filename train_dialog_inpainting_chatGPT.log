nohup: ignoring input
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/data/ccq/miniconda3/envs/llama/lib/python3.8/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/data/ccq/miniconda3/envs/llama/lib/python3.8/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/data/ccq/miniconda3/envs/llama/lib/python3.8/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
09/28/2023 15:28:58 - WARNING - llmtuner.tuner.core.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
/data/ccq/miniconda3/envs/llama/lib/python3.8/site-packages/transformers/training_args.py:1677: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ğŸ¤— Transformers. Use `--hub_token` instead.
  warnings.warn(
09/28/2023 15:28:58 - WARNING - llmtuner.tuner.core.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
09/28/2023 15:28:58 - INFO - llmtuner.tuner.core.parser - Process rank: 2, device: cuda:2, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
09/28/2023 15:28:58 - INFO - llmtuner.tuner.core.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=2,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output/dialog_inpainting_Baichuan2_chatGPT_2/runs/Sep28_15-28-57_stresstest,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=20.0,
optim=adamw_torch,
optim_args=None,
output_dir=output/dialog_inpainting_Baichuan2_chatGPT_2,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=output/dialog_inpainting_Baichuan2_chatGPT_2,
save_on_each_node=False,
save_safetensors=False,
save_steps=200,
save_strategy=steps,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
sortish_sampler=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
09/28/2023 15:28:58 - INFO - llmtuner.dsets.loader - Loading dataset dialog_inpainting_zh.json...
09/28/2023 15:28:58 - WARNING - llmtuner.dsets.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.
/data/ccq/miniconda3/envs/llama/lib/python3.8/site-packages/transformers/training_args.py:1677: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ğŸ¤— Transformers. Use `--hub_token` instead.
  warnings.warn(
/data/ccq/miniconda3/envs/llama/lib/python3.8/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=None' instead.
  warnings.warn(
09/28/2023 15:28:58 - WARNING - llmtuner.tuner.core.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
[INFO|training_args.py:1332] 2023-09-28 15:28:58,165 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!
[INFO|training_args.py:1764] 2023-09-28 15:28:58,165 >> PyTorch: setting up devices
09/28/2023 15:28:58 - INFO - llmtuner.tuner.core.parser - Process rank: 1, device: cuda:1, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
/data/ccq/miniconda3/envs/llama/lib/python3.8/site-packages/transformers/training_args.py:1677: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ğŸ¤— Transformers. Use `--hub_token` instead.
  warnings.warn(
09/28/2023 15:28:58 - INFO - llmtuner.tuner.core.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=1,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output/dialog_inpainting_Baichuan2_chatGPT_2/runs/Sep28_15-28-57_stresstest,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=20.0,
optim=adamw_torch,
optim_args=None,
output_dir=output/dialog_inpainting_Baichuan2_chatGPT_2,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=output/dialog_inpainting_Baichuan2_chatGPT_2,
save_on_each_node=False,
save_safetensors=False,
save_steps=200,
save_strategy=steps,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
sortish_sampler=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
09/28/2023 15:28:58 - INFO - llmtuner.dsets.loader - Loading dataset dialog_inpainting_zh.json...
09/28/2023 15:28:58 - WARNING - llmtuner.dsets.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.
/data/ccq/miniconda3/envs/llama/lib/python3.8/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=None' instead.
  warnings.warn(
09/28/2023 15:28:58 - INFO - llmtuner.tuner.core.parser - Process rank: 0, device: cuda:0, n_gpu: 1
  distributed training: True, compute dtype: torch.float16
09/28/2023 15:28:58 - INFO - llmtuner.tuner.core.parser - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=False,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output/dialog_inpainting_Baichuan2_chatGPT_2/runs/Sep28_15-28-57_stresstest,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_type=cosine,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=20.0,
optim=adamw_torch,
optim_args=None,
output_dir=output/dialog_inpainting_Baichuan2_chatGPT_2,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=4,
predict_with_generate=False,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=output/dialog_inpainting_Baichuan2_chatGPT_2,
save_on_each_node=False,
save_safetensors=False,
save_steps=200,
save_strategy=steps,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
sortish_sampler=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
09/28/2023 15:28:58 - INFO - llmtuner.dsets.loader - Loading dataset dialog_inpainting_zh.json...
09/28/2023 15:28:58 - WARNING - llmtuner.dsets.utils - Checksum failed: missing SHA-1 hash value in dataset_info.json.
/data/ccq/miniconda3/envs/llama/lib/python3.8/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=None' instead.
  warnings.warn(
Using custom data configuration default-425280d4459586ca
Loading Dataset Infos from /data/ccq/miniconda3/envs/llama/lib/python3.8/site-packages/datasets/packaged_modules/json
Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 3728.27it/s]
Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 979.75it/s]
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1264 examples [00:00, 44126.32 examples/s]
Found cached dataset json (/home/ccq/.cache/huggingface/datasets/json/default-425280d4459586ca/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
Loading Dataset info from /home/ccq/.cache/huggingface/datasets/json/default-425280d4459586ca/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96
[INFO|tokenization_utils_base.py:1850] 2023-09-28 15:28:59,265 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1850] 2023-09-28 15:28:59,265 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1850] 2023-09-28 15:28:59,265 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1850] 2023-09-28 15:28:59,265 >> loading file tokenizer_config.json
09/28/2023 15:28:59 - INFO - llmtuner.tuner.core.loader - Quantizing model to 4 bit.
09/28/2023 15:28:59 - INFO - llmtuner.tuner.core.loader - Quantizing model to 4 bit.
[INFO|configuration_utils.py:713] 2023-09-28 15:28:59,348 >> loading configuration file /data/ccq/models/Baichuan2-13B-Chat/config.json
[INFO|configuration_utils.py:713] 2023-09-28 15:28:59,349 >> loading configuration file /data/ccq/models/Baichuan2-13B-Chat/config.json
[INFO|configuration_utils.py:775] 2023-09-28 15:28:59,349 >> Model config BaichuanConfig {
  "_from_model_config": true,
  "_name_or_path": "/data/ccq/models/Baichuan2-13B-Chat",
  "architectures": [
    "BaichuanForCausalLM"
  ],
  "auto_map": {
    "AutoConfig": "configuration_baichuan.BaichuanConfig",
    "AutoModelForCausalLM": "modeling_baichuan.BaichuanForCausalLM"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "gradient_checkpointing": [
    false
  ],
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13696,
  "model_max_length": 4096,
  "model_type": "baichuan",
  "num_attention_heads": 40,
  "num_hidden_layers": 40,
  "pad_token_id": 0,
  "rms_norm_eps": 1e-06,
  "tie_word_embeddings": false,
  "tokenizer_class": "BaichuanTokenizer",
  "torch_dtype": "bfloat16",
  "transformers_version": "4.33.2",
  "use_cache": true,
  "vocab_size": 125696,
  "z_loss_weight": 0
}

09/28/2023 15:28:59 - INFO - llmtuner.tuner.core.loader - Quantizing model to 4 bit.
Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers
pip install xformers.
Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers
pip install xformers.
Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers
pip install xformers.
[INFO|modeling_utils.py:2866] 2023-09-28 15:28:59,378 >> loading weights file /data/ccq/models/Baichuan2-13B-Chat/pytorch_model.bin.index.json
[INFO|modeling_utils.py:1200] 2023-09-28 15:28:59,378 >> Instantiating BaichuanForCausalLM model under default dtype torch.float16.
[INFO|configuration_utils.py:768] 2023-09-28 15:28:59,379 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.33.2"
}

[INFO|modeling_utils.py:2983] 2023-09-28 15:28:59,466 >> Detected 4-bit loading: activating 4-bit loading for this model
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:17<00:34, 17.40s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:18<00:36, 18.12s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:18<00:36, 18.25s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:33<00:16, 16.44s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:34<00:16, 16.86s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:34<00:17, 17.08s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:46<00:00, 14.96s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:46<00:00, 15.46s/it]
09/28/2023 15:29:46 - INFO - llmtuner.tuner.core.adapter - Fine-tuning method: LoRA
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:47<00:00, 15.39s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:47<00:00, 15.91s/it]
[INFO|modeling_utils.py:3655] 2023-09-28 15:29:47,436 >> All model checkpoint weights were used when initializing BaichuanForCausalLM.

[INFO|modeling_utils.py:3663] 2023-09-28 15:29:47,436 >> All the weights of BaichuanForCausalLM were initialized from the model checkpoint at /data/ccq/models/Baichuan2-13B-Chat.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BaichuanForCausalLM for predictions without further training.
[INFO|configuration_utils.py:728] 2023-09-28 15:29:47,439 >> loading configuration file /data/ccq/models/Baichuan2-13B-Chat/generation_config.json
[INFO|configuration_utils.py:768] 2023-09-28 15:29:47,439 >> Generate config GenerationConfig {
  "assistant_token_id": 196,
  "bos_token_id": 1,
  "do_sample": true,
  "eos_token_id": 2,
  "max_new_tokens": 2048,
  "pad_token_id": 0,
  "repetition_penalty": 1.05,
  "temperature": 0.3,
  "top_k": 5,
  "top_p": 0.85,
  "transformers_version": "4.33.2",
  "user_token_id": 195
}

09/28/2023 15:29:47 - INFO - llmtuner.tuner.core.adapter - Fine-tuning method: LoRA
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:49<00:00, 16.09s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:49<00:00, 16.48s/it]
09/28/2023 15:29:49 - INFO - llmtuner.tuner.core.adapter - Fine-tuning method: LoRA
09/28/2023 15:30:08 - INFO - llmtuner.tuner.core.loader - trainable params: 6553600 || all params: 13903221760 || trainable%: 0.0471
09/28/2023 15:30:10 - INFO - llmtuner.tuner.core.loader - trainable params: 6553600 || all params: 13903221760 || trainable%: 0.0471
[INFO|tokenization_utils_base.py:926] 2023-09-28 15:30:10,648 >> Assigning [] to the additional_special_tokens key of the tokenizer
Running tokenizer on dataset:   0%|          | 0/1264 [00:00<?, ? examples/s]Caching processed dataset at /home/ccq/.cache/huggingface/datasets/json/default-425280d4459586ca/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-fd6601c40f6c14dd.arrow
Running tokenizer on dataset:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1000/1264 [00:00<00:00, 1044.66 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1264/1264 [00:01<00:00, 1069.09 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1264/1264 [00:01<00:00, 1061.22 examples/s]
input_ids:
[92335, 92345, 92655, 20344, 2842, 92574, 5, 92336, 92345, 18496, 2454, 2841, 70, 5, 63, 100311, 92360, 47477, 92361, 24562, 13084, 17220, 7851, 93410, 65, 92594, 74234, 4962, 21847, 65, 92587, 17220, 4885, 12221, 59734, 92895, 93410, 66, 5, 63, 100270, 20617, 20967, 17220, 65, 15796, 94339, 93081, 28348, 94752, 92333, 13264, 2017, 3712, 92895, 65, 16578, 5719, 32094, 66, 2350, 18496, 17220, 17904, 92609, 51187, 93410, 8705, 68, 2]
inputs:
 0:<QUES>
1:çº æ­£æ–¹æ³•åŒ…æ‹¬ï¼š
- ç”¨â€œåä¸‹â€å£ä»¤å‘½ä»¤ç‹—ç‹—åœæ­¢å«ï¼Œå¹¶æ¡ä½å®ƒçš„å˜´å·´ï¼Œè®©ç‹—ç‹—æ˜ç™½ä¸»äººä¸å¸Œæœ›å®ƒå«ã€‚
- å¯¹äºå†²åŠ¨å‹çš„ç‹—ç‹—ï¼Œå¯ä»¥ç”¨çŒ›æ‹‰ç‰µå¼•ç»³çš„æ¿€çƒˆæ–¹å¼å‘Šè¯‰å®ƒï¼Œè¿™æ ·åšæ˜¯ä¸å¯¹çš„ã€‚å¦‚ä½•çº æ­£ç‹—ç‹—å¯¹ç€é—¨ä½¿åŠ²å«çš„è¡Œä¸ºï¼Ÿ</s>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2350, 18496, 17220, 17904, 92609, 51187, 93410, 8705, 68, 2]
labels:
 å¦‚ä½•çº æ­£ç‹—ç‹—å¯¹ç€é—¨ä½¿åŠ²å«çš„è¡Œä¸ºï¼Ÿ</s>
09/28/2023 15:30:12 - INFO - llmtuner.tuner.core.loader - trainable params: 6553600 || all params: 13903221760 || trainable%: 0.0471
[INFO|training_args.py:1332] 2023-09-28 15:30:12,931 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!
[INFO|training_args.py:1764] 2023-09-28 15:30:12,931 >> PyTorch: setting up devices
[INFO|trainer.py:403] 2023-09-28 15:30:12,933 >> The model is quantized. To train this model you need to add additional modules inside the model such as adapters using `peft` library and freeze the model weights. Please check the examples in https://github.com/huggingface/peft for more details.
Running tokenizer on dataset:   0%|          | 0/1264 [00:00<?, ? examples/s]Running tokenizer on dataset:   0%|          | 0/1264 [00:00<?, ? examples/s]Running tokenizer on dataset:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1000/1264 [00:00<00:00, 1031.45 examples/s]Running tokenizer on dataset:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1000/1264 [00:00<00:00, 1025.66 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1264/1264 [00:01<00:00, 1051.86 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1264/1264 [00:01<00:00, 1052.45 examples/s]Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1264/1264 [00:01<00:00, 1045.14 examples/s]
input_ids:
[92335, 92345, 92655, 20344, 2842, 92574, 5, 92336, 92345, 18496, 2454, 2841, 70, 5, 63, 100311, 92360, 47477, 92361, 24562, 13084, 17220, 7851, 93410, 65, 92594, 74234, 4962, 21847, 65, 92587, 17220, 4885, 12221, 59734, 92895, 93410, 66, 5, 63, 100270, 20617, 20967, 17220, 65, 15796, 94339, 93081, 28348, 94752, 92333, 13264, 2017, 3712, 92895, 65, 16578, 5719, 32094, 66, 2350, 18496, 17220, 17904, 92609, 51187, 93410, 8705, 68, 2]
inputs:
 0:<QUES>
1:çº æ­£æ–¹æ³•åŒ…æ‹¬ï¼š
- ç”¨â€œåä¸‹â€å£ä»¤å‘½ä»¤ç‹—ç‹—åœæ­¢å«ï¼Œå¹¶æ¡ä½å®ƒçš„å˜´å·´ï¼Œè®©ç‹—ç‹—æ˜ç™½ä¸»äººä¸å¸Œæœ›å®ƒå«ã€‚
- å¯¹äºå†²åŠ¨å‹çš„ç‹—ç‹—ï¼Œå¯ä»¥ç”¨çŒ›æ‹‰ç‰µå¼•ç»³çš„æ¿€çƒˆæ–¹å¼å‘Šè¯‰å®ƒï¼Œè¿™æ ·åšæ˜¯ä¸å¯¹çš„ã€‚å¦‚ä½•çº æ­£ç‹—ç‹—å¯¹ç€é—¨ä½¿åŠ²å«çš„è¡Œä¸ºï¼Ÿ</s>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2350, 18496, 17220, 17904, 92609, 51187, 93410, 8705, 68, 2]
labels:
 å¦‚ä½•çº æ­£ç‹—ç‹—å¯¹ç€é—¨ä½¿åŠ²å«çš„è¡Œä¸ºï¼Ÿ</s>
Running tokenizer on dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1264/1264 [00:01<00:00, 1043.08 examples/s]
input_ids:
[92335, 92345, 92655, 20344, 2842, 92574, 5, 92336, 92345, 18496, 2454, 2841, 70, 5, 63, 100311, 92360, 47477, 92361, 24562, 13084, 17220, 7851, 93410, 65, 92594, 74234, 4962, 21847, 65, 92587, 17220, 4885, 12221, 59734, 92895, 93410, 66, 5, 63, 100270, 20617, 20967, 17220, 65, 15796, 94339, 93081, 28348, 94752, 92333, 13264, 2017, 3712, 92895, 65, 16578, 5719, 32094, 66, 2350, 18496, 17220, 17904, 92609, 51187, 93410, 8705, 68, 2]
inputs:
 0:<QUES>
1:çº æ­£æ–¹æ³•åŒ…æ‹¬ï¼š
- ç”¨â€œåä¸‹â€å£ä»¤å‘½ä»¤ç‹—ç‹—åœæ­¢å«ï¼Œå¹¶æ¡ä½å®ƒçš„å˜´å·´ï¼Œè®©ç‹—ç‹—æ˜ç™½ä¸»äººä¸å¸Œæœ›å®ƒå«ã€‚
- å¯¹äºå†²åŠ¨å‹çš„ç‹—ç‹—ï¼Œå¯ä»¥ç”¨çŒ›æ‹‰ç‰µå¼•ç»³çš„æ¿€çƒˆæ–¹å¼å‘Šè¯‰å®ƒï¼Œè¿™æ ·åšæ˜¯ä¸å¯¹çš„ã€‚å¦‚ä½•çº æ­£ç‹—ç‹—å¯¹ç€é—¨ä½¿åŠ²å«çš„è¡Œä¸ºï¼Ÿ</s>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2350, 18496, 17220, 17904, 92609, 51187, 93410, 8705, 68, 2]
labels:
 å¦‚ä½•çº æ­£ç‹—ç‹—å¯¹ç€é—¨ä½¿åŠ²å«çš„è¡Œä¸ºï¼Ÿ</s>
[INFO|trainer.py:1712] 2023-09-28 15:30:14,457 >> ***** Running training *****
[INFO|trainer.py:1713] 2023-09-28 15:30:14,457 >>   Num examples = 1,264
[INFO|trainer.py:1714] 2023-09-28 15:30:14,457 >>   Num Epochs = 20
[INFO|trainer.py:1715] 2023-09-28 15:30:14,457 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:1718] 2023-09-28 15:30:14,457 >>   Total train batch size (w. parallel, distributed & accumulation) = 48
[INFO|trainer.py:1719] 2023-09-28 15:30:14,457 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:1720] 2023-09-28 15:30:14,457 >>   Total optimization steps = 520
[INFO|trainer.py:1721] 2023-09-28 15:30:14,459 >>   Number of trainable parameters = 6,553,600
  0%|          | 0/520 [00:00<?, ?it/s]  0%|          | 1/520 [00:17<2:29:28, 17.28s/it]  0%|          | 2/520 [00:39<2:51:54, 19.91s/it]  1%|          | 3/520 [00:54<2:32:29, 17.70s/it]  1%|          | 4/520 [01:09<2:25:43, 16.95s/it]  1%|          | 5/520 [01:26<2:24:48, 16.87s/it]  1%|          | 6/520 [01:40<2:14:39, 15.72s/it]  1%|â–         | 7/520 [02:01<2:30:28, 17.60s/it]  2%|â–         | 8/520 [02:20<2:32:27, 17.87s/it]  2%|â–         | 9/520 [02:40<2:39:40, 18.75s/it]  2%|â–         | 10/520 [03:01<2:44:45, 19.38s/it]                                                  {'loss': 2.8901, 'learning_rate': 4.997080567080817e-05, 'epoch': 0.38}
  2%|â–         | 10/520 [03:01<2:44:45, 19.38s/it]  2%|â–         | 11/520 [03:18<2:38:28, 18.68s/it]  2%|â–         | 12/520 [03:37<2:37:32, 18.61s/it]  2%|â–         | 13/520 [03:52<2:29:31, 17.70s/it]  3%|â–         | 14/520 [04:12<2:33:39, 18.22s/it]  3%|â–         | 15/520 [04:30<2:34:41, 18.38s/it]  3%|â–         | 16/520 [04:48<2:32:53, 18.20s/it]  3%|â–         | 17/520 [05:03<2:23:05, 17.07s/it]  3%|â–         | 18/520 [05:22<2:28:26, 17.74s/it]  4%|â–         | 19/520 [05:41<2:31:54, 18.19s/it]  4%|â–         | 20/520 [05:58<2:28:37, 17.84s/it]                                                  {'loss': 2.143, 'learning_rate': 4.985232055833313e-05, 'epoch': 0.75}
  4%|â–         | 20/520 [05:58<2:28:37, 17.84s/it]  4%|â–         | 21/520 [06:14<2:24:13, 17.34s/it]  4%|â–         | 22/520 [06:28<2:13:58, 16.14s/it]  4%|â–         | 23/520 [06:44<2:13:25, 16.11s/it]  5%|â–         | 24/520 [07:02<2:19:27, 16.87s/it]  5%|â–         | 25/520 [07:21<2:23:10, 17.35s/it]  5%|â–Œ         | 26/520 [07:36<2:18:42, 16.85s/it]  5%|â–Œ         | 27/520 [07:56<2:24:41, 17.61s/it]  5%|â–Œ         | 28/520 [08:15<2:28:30, 18.11s/it]  6%|â–Œ         | 29/520 [08:36<2:35:54, 19.05s/it]  6%|â–Œ         | 30/520 [08:58<2:41:14, 19.74s/it]                                                  {'loss': 1.6811, 'learning_rate': 4.964315202329127e-05, 'epoch': 1.13}
  6%|â–Œ         | 30/520 [08:58<2:41:14, 19.74s/it]  6%|â–Œ         | 31/520 [09:17<2:40:27, 19.69s/it]  6%|â–Œ         | 32/520 [09:31<2:24:54, 17.82s/it]  6%|â–‹         | 33/520 [09:45<2:16:02, 16.76s/it]  7%|â–‹         | 34/520 [10:01<2:12:47, 16.39s/it]  7%|â–‹         | 35/520 [10:20<2:19:34, 17.27s/it]  7%|â–‹         | 36/520 [10:37<2:19:13, 17.26s/it]  7%|â–‹         | 37/520 [10:51<2:11:23, 16.32s/it]  7%|â–‹         | 38/520 [11:08<2:12:19, 16.47s/it]  8%|â–Š         | 39/520 [11:27<2:17:33, 17.16s/it]  8%|â–Š         | 40/520 [11:48<2:27:19, 18.42s/it]                                                  {'loss': 1.4443, 'learning_rate': 4.934406329898334e-05, 'epoch': 1.51}
  8%|â–Š         | 40/520 [11:48<2:27:19, 18.42s/it]  8%|â–Š         | 41/520 [12:05<2:24:09, 18.06s/it]  8%|â–Š         | 42/520 [12:25<2:28:31, 18.64s/it]  8%|â–Š         | 43/520 [12:46<2:32:26, 19.17s/it]  8%|â–Š         | 44/520 [13:00<2:19:28, 17.58s/it]  9%|â–Š         | 45/520 [13:16<2:17:11, 17.33s/it]  9%|â–‰         | 46/520 [13:37<2:24:14, 18.26s/it]  9%|â–‰         | 47/520 [13:52<2:15:56, 17.24s/it]  9%|â–‰         | 48/520 [14:09<2:16:49, 17.39s/it]  9%|â–‰         | 49/520 [14:25<2:12:54, 16.93s/it] 10%|â–‰         | 50/520 [14:43<2:14:41, 17.20s/it]                                                  {'loss': 1.4033, 'learning_rate': 4.8956145727729156e-05, 'epoch': 1.89}
 10%|â–‰         | 50/520 [14:43<2:14:41, 17.20s/it] 10%|â–‰         | 51/520 [15:03<2:21:12, 18.07s/it] 10%|â–ˆ         | 52/520 [15:20<2:18:10, 17.71s/it] 10%|â–ˆ         | 53/520 [15:36<2:14:41, 17.31s/it] 10%|â–ˆ         | 54/520 [15:54<2:16:02, 17.52s/it] 11%|â–ˆ         | 55/520 [16:11<2:12:35, 17.11s/it] 11%|â–ˆ         | 56/520 [16:27<2:11:35, 17.02s/it] 11%|â–ˆ         | 57/520 [16:43<2:08:43, 16.68s/it] 11%|â–ˆ         | 58/520 [16:59<2:05:15, 16.27s/it] 11%|â–ˆâ–        | 59/520 [17:18<2:11:44, 17.15s/it] 12%|â–ˆâ–        | 60/520 [17:37<2:16:27, 17.80s/it]                                                  {'loss': 1.2507, 'learning_rate': 4.8480814778677886e-05, 'epoch': 2.26}
 12%|â–ˆâ–        | 60/520 [17:37<2:16:27, 17.80s/it] 12%|â–ˆâ–        | 61/520 [17:54<2:14:11, 17.54s/it] 12%|â–ˆâ–        | 62/520 [18:14<2:18:54, 18.20s/it] 12%|â–ˆâ–        | 63/520 [18:37<2:28:49, 19.54s/it] 12%|â–ˆâ–        | 64/520 [18:52<2:19:33, 18.36s/it] 12%|â–ˆâ–        | 65/520 [19:08<2:13:32, 17.61s/it] 13%|â–ˆâ–        | 66/520 [19:25<2:11:38, 17.40s/it] 13%|â–ˆâ–        | 67/520 [19:41<2:08:02, 16.96s/it] 13%|â–ˆâ–        | 68/520 [19:54<2:00:10, 15.95s/it] 13%|â–ˆâ–        | 69/520 [20:10<1:59:24, 15.89s/it] 13%|â–ˆâ–        | 70/520 [20:28<2:04:01, 16.54s/it]                                                  {'loss': 1.2469, 'learning_rate': 4.791980488291456e-05, 'epoch': 2.64}
 13%|â–ˆâ–        | 70/520 [20:28<2:04:01, 16.54s/it] 14%|â–ˆâ–        | 71/520 [20:49<2:14:22, 17.96s/it] 14%|â–ˆâ–        | 72/520 [21:04<2:06:28, 16.94s/it] 14%|â–ˆâ–        | 73/520 [21:22<2:08:26, 17.24s/it] 14%|â–ˆâ–        | 74/520 [21:39<2:07:53, 17.21s/it] 14%|â–ˆâ–        | 75/520 [21:54<2:03:22, 16.63s/it] 15%|â–ˆâ–        | 76/520 [22:14<2:09:55, 17.56s/it] 15%|â–ˆâ–        | 77/520 [22:29<2:04:47, 16.90s/it] 15%|â–ˆâ–Œ        | 78/520 [22:49<2:09:22, 17.56s/it] 15%|â–ˆâ–Œ        | 79/520 [23:09<2:16:03, 18.51s/it] 15%|â–ˆâ–Œ        | 80/520 [23:26<2:10:46, 17.83s/it]                                                  {'loss': 1.1881, 'learning_rate': 4.72751631047092e-05, 'epoch': 3.02}
 15%|â–ˆâ–Œ        | 80/520 [23:26<2:10:46, 17.83s/it] 16%|â–ˆâ–Œ        | 81/520 [23:44<2:11:37, 17.99s/it] 16%|â–ˆâ–Œ        | 82/520 [24:01<2:08:49, 17.65s/it] 16%|â–ˆâ–Œ        | 83/520 [24:20<2:11:15, 18.02s/it] 16%|â–ˆâ–Œ        | 84/520 [24:37<2:10:04, 17.90s/it] 16%|â–ˆâ–‹        | 85/520 [24:55<2:09:23, 17.85s/it] 17%|â–ˆâ–‹        | 86/520 [25:14<2:10:50, 18.09s/it] 17%|â–ˆâ–‹        | 87/520 [25:32<2:10:48, 18.13s/it] 17%|â–ˆâ–‹        | 88/520 [25:45<1:59:28, 16.59s/it] 17%|â–ˆâ–‹        | 89/520 [26:05<2:07:40, 17.77s/it] 17%|â–ˆâ–‹        | 90/520 [26:23<2:05:52, 17.56s/it]                                                  {'loss': 1.0983, 'learning_rate': 4.654924167200123e-05, 'epoch': 3.4}
 17%|â–ˆâ–‹        | 90/520 [26:23<2:05:52, 17.56s/it] 18%|â–ˆâ–Š        | 91/520 [26:39<2:02:53, 17.19s/it] 18%|â–ˆâ–Š        | 92/520 [26:54<1:57:39, 16.49s/it] 18%|â–ˆâ–Š        | 93/520 [27:15<2:06:37, 17.79s/it] 18%|â–ˆâ–Š        | 94/520 [27:32<2:05:29, 17.68s/it] 18%|â–ˆâ–Š        | 95/520 [27:52<2:10:44, 18.46s/it] 18%|â–ˆâ–Š        | 96/520 [28:11<2:10:06, 18.41s/it] 19%|â–ˆâ–Š        | 97/520 [28:26<2:04:35, 17.67s/it] 19%|â–ˆâ–‰        | 98/520 [28:47<2:09:25, 18.40s/it] 19%|â–ˆâ–‰        | 99/520 [29:01<2:01:07, 17.26s/it] 19%|â–ˆâ–‰        | 100/520 [29:20<2:03:32, 17.65s/it]                                                   {'loss': 1.0965, 'learning_rate': 4.574468939337488e-05, 'epoch': 3.77}
 19%|â–ˆâ–‰        | 100/520 [29:20<2:03:32, 17.65s/it] 19%|â–ˆâ–‰        | 101/520 [29:34<1:55:43, 16.57s/it] 20%|â–ˆâ–‰        | 102/520 [29:51<1:57:51, 16.92s/it] 20%|â–ˆâ–‰        | 103/520 [30:10<1:59:57, 17.26s/it] 20%|â–ˆâ–ˆ        | 104/520 [30:29<2:04:54, 18.02s/it] 20%|â–ˆâ–ˆ        | 105/520 [30:51<2:13:10, 19.25s/it] 20%|â–ˆâ–ˆ        | 106/520 [31:08<2:07:28, 18.47s/it] 21%|â–ˆâ–ˆ        | 107/520 [31:24<2:02:38, 17.82s/it] 21%|â–ˆâ–ˆ        | 108/520 [31:45<2:07:46, 18.61s/it] 21%|â–ˆâ–ˆ        | 109/520 [32:00<2:01:16, 17.70s/it] 21%|â–ˆâ–ˆ        | 110/520 [32:20<2:04:12, 18.18s/it]                                                   {'loss': 1.0889, 'learning_rate': 4.486444199284386e-05, 'epoch': 4.15}
 21%|â–ˆâ–ˆ        | 110/520 [32:20<2:04:12, 18.18s/it] 21%|â–ˆâ–ˆâ–       | 111/520 [32:41<2:10:01, 19.08s/it] 22%|â–ˆâ–ˆâ–       | 112/520 [32:56<2:01:59, 17.94s/it] 22%|â–ˆâ–ˆâ–       | 113/520 [33:12<1:58:12, 17.43s/it] 22%|â–ˆâ–ˆâ–       | 114/520 [33:30<1:58:49, 17.56s/it] 22%|â–ˆâ–ˆâ–       | 115/520 [33:46<1:55:06, 17.05s/it] 22%|â–ˆâ–ˆâ–       | 116/520 [34:02<1:51:21, 16.54s/it] 22%|â–ˆâ–ˆâ–       | 117/520 [34:19<1:52:19, 16.72s/it] 23%|â–ˆâ–ˆâ–       | 118/520 [34:37<1:55:17, 17.21s/it] 23%|â–ˆâ–ˆâ–       | 119/520 [34:59<2:04:22, 18.61s/it] 23%|â–ˆâ–ˆâ–       | 120/520 [35:15<1:59:55, 17.99s/it]                                                   {'loss': 1.0281, 'learning_rate': 4.391171139771284e-05, 'epoch': 4.53}
 23%|â–ˆâ–ˆâ–       | 120/520 [35:15<1:59:55, 17.99s/it] 23%|â–ˆâ–ˆâ–       | 121/520 [35:30<1:53:08, 17.01s/it] 23%|â–ˆâ–ˆâ–       | 122/520 [35:44<1:47:07, 16.15s/it] 24%|â–ˆâ–ˆâ–       | 123/520 [36:00<1:45:13, 15.90s/it] 24%|â–ˆâ–ˆâ–       | 124/520 [36:20<1:53:27, 17.19s/it] 24%|â–ˆâ–ˆâ–       | 125/520 [36:40<1:58:27, 17.99s/it] 24%|â–ˆâ–ˆâ–       | 126/520 [36:57<1:57:17, 17.86s/it] 24%|â–ˆâ–ˆâ–       | 127/520 [37:20<2:05:55, 19.22s/it] 25%|â–ˆâ–ˆâ–       | 128/520 [37:38<2:04:15, 19.02s/it] 25%|â–ˆâ–ˆâ–       | 129/520 [37:57<2:04:14, 19.07s/it] 25%|â–ˆâ–ˆâ–Œ       | 130/520 [38:16<2:03:44, 19.04s/it]                                                   {'loss': 0.9525, 'learning_rate': 4.299514872088788e-05, 'epoch': 4.91}
 25%|â–ˆâ–ˆâ–Œ       | 130/520 [38:16<2:03:44, 19.04s/it] 25%|â–ˆâ–ˆâ–Œ       | 131/520 [38:35<2:03:02, 18.98s/it] 25%|â–ˆâ–ˆâ–Œ       | 132/520 [38:52<1:58:54, 18.39s/it] 26%|â–ˆâ–ˆâ–Œ       | 133/520 [39:09<1:56:03, 17.99s/it] 26%|â–ˆâ–ˆâ–Œ       | 134/520 [39:26<1:52:43, 17.52s/it] 26%|â–ˆâ–ˆâ–Œ       | 135/520 [39:47<1:59:54, 18.69s/it] 26%|â–ˆâ–ˆâ–Œ       | 136/520 [40:09<2:06:24, 19.75s/it] 26%|â–ˆâ–ˆâ–‹       | 137/520 [40:26<1:59:16, 18.68s/it] 27%|â–ˆâ–ˆâ–‹       | 138/520 [40:40<1:50:34, 17.37s/it] 27%|â–ˆâ–ˆâ–‹       | 139/520 [41:00<1:55:02, 18.12s/it] 27%|â–ˆâ–ˆâ–‹       | 140/520 [41:18<1:55:43, 18.27s/it]                                                   {'loss': 0.9405, 'learning_rate': 4.1914485955605785e-05, 'epoch': 5.28}
 27%|â–ˆâ–ˆâ–‹       | 140/520 [41:18<1:55:43, 18.27s/it] 27%|â–ˆâ–ˆâ–‹       | 141/520 [41:34<1:50:58, 17.57s/it] 27%|â–ˆâ–ˆâ–‹       | 142/520 [41:45<1:38:43, 15.67s/it] 28%|â–ˆâ–ˆâ–Š       | 143/520 [42:11<1:57:52, 18.76s/it] 28%|â–ˆâ–ˆâ–Š       | 144/520 [42:29<1:56:05, 18.53s/it] 28%|â–ˆâ–ˆâ–Š       | 145/520 [42:52<2:03:16, 19.72s/it] 28%|â–ˆâ–ˆâ–Š       | 146/520 [43:13<2:06:17, 20.26s/it] 28%|â–ˆâ–ˆâ–Š       | 147/520 [43:32<2:01:49, 19.60s/it] 28%|â–ˆâ–ˆâ–Š       | 148/520 [43:47<1:54:11, 18.42s/it] 29%|â–ˆâ–ˆâ–Š       | 149/520 [44:02<1:46:35, 17.24s/it] 29%|â–ˆâ–ˆâ–‰       | 150/520 [44:18<1:45:03, 17.04s/it]                                                   {'loss': 0.9303, 'learning_rate': 4.077210406539077e-05, 'epoch': 5.66}
 29%|â–ˆâ–ˆâ–‰       | 150/520 [44:18<1:45:03, 17.04s/it] 29%|â–ˆâ–ˆâ–‰       | 151/520 [44:33<1:39:41, 16.21s/it] 29%|â–ˆâ–ˆâ–‰       | 152/520 [44:49<1:40:07, 16.33s/it] 29%|â–ˆâ–ˆâ–‰       | 153/520 [45:07<1:43:17, 16.89s/it] 30%|â–ˆâ–ˆâ–‰       | 154/520 [45:29<1:52:37, 18.46s/it] 30%|â–ˆâ–ˆâ–‰       | 155/520 [45:50<1:55:39, 19.01s/it] 30%|â–ˆâ–ˆâ–ˆ       | 156/520 [46:16<2:08:13, 21.14s/it] 30%|â–ˆâ–ˆâ–ˆ       | 157/520 [46:36<2:05:24, 20.73s/it] 30%|â–ˆâ–ˆâ–ˆ       | 158/520 [46:51<1:54:50, 19.03s/it] 31%|â–ˆâ–ˆâ–ˆ       | 159/520 [47:05<1:45:36, 17.55s/it] 31%|â–ˆâ–ˆâ–ˆ       | 160/520 [47:20<1:41:49, 16.97s/it]                                                   {'loss': 0.933, 'learning_rate': 3.957217147787276e-05, 'epoch': 6.04}
 31%|â–ˆâ–ˆâ–ˆ       | 160/520 [47:20<1:41:49, 16.97s/it] 31%|â–ˆâ–ˆâ–ˆ       | 161/520 [47:36<1:39:13, 16.58s/it] 31%|â–ˆâ–ˆâ–ˆ       | 162/520 [47:49<1:33:06, 15.60s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 163/520 [48:08<1:38:43, 16.59s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 164/520 [48:23<1:35:35, 16.11s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 165/520 [48:43<1:41:13, 17.11s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 166/520 [49:01<1:42:43, 17.41s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 167/520 [49:18<1:41:24, 17.24s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 168/520 [49:34<1:40:07, 17.07s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 169/520 [49:52<1:41:39, 17.38s/it] 33%|â–ˆâ–ˆâ–ˆâ–      | 170/520 [50:07<1:35:42, 16.41s/it]                                                   {'loss': 0.8755, 'learning_rate': 3.831906661693494e-05, 'epoch': 6.42}
 33%|â–ˆâ–ˆâ–ˆâ–      | 170/520 [50:07<1:35:42, 16.41s/it] 33%|â–ˆâ–ˆâ–ˆâ–      | 171/520 [50:23<1:35:45, 16.46s/it] 33%|â–ˆâ–ˆâ–ˆâ–      | 172/520 [50:37<1:30:26, 15.59s/it] 33%|â–ˆâ–ˆâ–ˆâ–      | 173/520 [50:54<1:32:52, 16.06s/it] 33%|â–ˆâ–ˆâ–ˆâ–      | 174/520 [51:16<1:43:23, 17.93s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 175/520 [51:32<1:39:57, 17.38s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 176/520 [51:50<1:39:23, 17.33s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 177/520 [52:06<1:38:16, 17.19s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 178/520 [52:23<1:37:34, 17.12s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 179/520 [52:42<1:39:31, 17.51s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 180/520 [53:01<1:41:46, 17.96s/it]                                                   {'loss': 0.8268, 'learning_rate': 3.7017361926319896e-05, 'epoch': 6.79}
 35%|â–ˆâ–ˆâ–ˆâ–      | 180/520 [53:01<1:41:46, 17.96s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 181/520 [53:16<1:37:03, 17.18s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 182/520 [53:33<1:35:43, 16.99s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 183/520 [53:48<1:31:49, 16.35s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 184/520 [54:06<1:35:56, 17.13s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 185/520 [54:25<1:38:47, 17.69s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 186/520 [54:44<1:40:38, 18.08s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 187/520 [55:04<1:42:51, 18.53s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 188/520 [55:19<1:37:09, 17.56s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 189/520 [55:39<1:40:22, 18.19s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 190/520 [56:00<1:43:57, 18.90s/it]                                                   {'loss': 0.8072, 'learning_rate': 3.567180718527816e-05, 'epoch': 7.17}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 190/520 [56:00<1:43:57, 18.90s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 191/520 [56:18<1:43:40, 18.91s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 192/520 [56:38<1:44:39, 19.15s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 193/520 [56:52<1:35:23, 17.50s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 194/520 [57:08<1:32:40, 17.06s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 195/520 [57:23<1:29:26, 16.51s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 196/520 [57:37<1:25:12, 15.78s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 197/520 [57:52<1:23:27, 15.50s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 198/520 [58:12<1:30:04, 16.78s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 199/520 [58:33<1:36:23, 18.02s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 200/520 [58:50<1:34:21, 17.69s/it]                                                   {'loss': 0.793, 'learning_rate': 3.442737104220801e-05, 'epoch': 7.55}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 200/520 [58:50<1:34:21, 17.69s/it][INFO|trainer.py:2841] 2023-09-28 16:29:06,081 >> Saving model checkpoint to output/dialog_inpainting_Baichuan2_chatGPT_2/checkpoint-200
[INFO|tokenization_utils_base.py:2235] 2023-09-28 16:29:06,133 >> tokenizer config file saved in output/dialog_inpainting_Baichuan2_chatGPT_2/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2242] 2023-09-28 16:29:06,133 >> Special tokens file saved in output/dialog_inpainting_Baichuan2_chatGPT_2/checkpoint-200/special_tokens_map.json
 39%|â–ˆâ–ˆâ–ˆâ–Š      | 201/520 [59:07<1:33:05, 17.51s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 202/520 [59:25<1:34:17, 17.79s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 203/520 [59:38<1:26:48, 16.43s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 204/520 [59:54<1:24:36, 16.06s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 205/520 [1:00:13<1:28:52, 16.93s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 206/520 [1:00:31<1:30:28, 17.29s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 207/520 [1:00:51<1:35:25, 18.29s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 208/520 [1:01:09<1:34:52, 18.24s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 209/520 [1:01:26<1:32:00, 17.75s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 210/520 [1:01:42<1:29:08, 17.25s/it]                                                     {'loss': 0.7601, 'learning_rate': 3.301214547355774e-05, 'epoch': 7.92}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 210/520 [1:01:42<1:29:08, 17.25s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 211/520 [1:02:00<1:29:44, 17.43s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 212/520 [1:02:19<1:31:33, 17.84s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 213/520 [1:02:38<1:33:32, 18.28s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 214/520 [1:02:56<1:33:21, 18.30s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 215/520 [1:03:17<1:36:20, 18.95s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 216/520 [1:03:30<1:27:13, 17.21s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 217/520 [1:03:47<1:26:39, 17.16s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 218/520 [1:04:03<1:24:43, 16.83s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 219/520 [1:04:19<1:23:13, 16.59s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 220/520 [1:04:33<1:19:19, 15.86s/it]                                                     {'loss': 0.7283, 'learning_rate': 3.1567684454964675e-05, 'epoch': 8.3}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 220/520 [1:04:33<1:19:19, 15.86s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 221/520 [1:04:51<1:22:19, 16.52s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 222/520 [1:05:10<1:24:56, 17.10s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 223/520 [1:05:26<1:23:33, 16.88s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 224/520 [1:05:40<1:18:12, 15.85s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 225/520 [1:05:56<1:18:13, 15.91s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 226/520 [1:06:12<1:19:01, 16.13s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 227/520 [1:06:28<1:17:16, 15.82s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 228/520 [1:06:46<1:21:20, 16.72s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 229/520 [1:07:04<1:22:28, 17.00s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 230/520 [1:07:19<1:19:06, 16.37s/it]                                                     {'loss': 0.7155, 'learning_rate': 3.009925866803872e-05, 'epoch': 8.68}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 230/520 [1:07:19<1:19:06, 16.37s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 231/520 [1:07:32<1:14:23, 15.45s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 232/520 [1:07:54<1:23:35, 17.42s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 233/520 [1:08:14<1:26:31, 18.09s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 234/520 [1:08:32<1:26:29, 18.14s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 235/520 [1:08:48<1:22:20, 17.34s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 236/520 [1:09:11<1:30:54, 19.21s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 237/520 [1:09:33<1:34:56, 20.13s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 238/520 [1:09:49<1:27:50, 18.69s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 239/520 [1:10:02<1:19:48, 17.04s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 240/520 [1:10:19<1:19:29, 17.03s/it]                                                     {'loss': 0.6422, 'learning_rate': 2.8612226239230532e-05, 'epoch': 9.06}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 240/520 [1:10:19<1:19:29, 17.03s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 241/520 [1:10:31<1:12:51, 15.67s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 242/520 [1:10:52<1:18:55, 17.03s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 243/520 [1:11:09<1:18:40, 17.04s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 244/520 [1:11:26<1:18:37, 17.09s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 245/520 [1:11:45<1:21:21, 17.75s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 246/520 [1:12:01<1:17:55, 17.06s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 247/520 [1:12:17<1:16:43, 16.86s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 248/520 [1:12:36<1:19:17, 17.49s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 249/520 [1:12:55<1:21:07, 17.96s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 250/520 [1:13:13<1:21:13, 18.05s/it]                                                     {'loss': 0.6227, 'learning_rate': 2.711201318860918e-05, 'epoch': 9.43}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 250/520 [1:13:13<1:21:13, 18.05s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 251/520 [1:13:29<1:17:32, 17.30s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 252/520 [1:13:48<1:19:14, 17.74s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 253/520 [1:14:07<1:21:04, 18.22s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 254/520 [1:14:25<1:20:39, 18.19s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 255/520 [1:14:43<1:19:41, 18.04s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 256/520 [1:15:01<1:19:14, 18.01s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 257/520 [1:15:19<1:19:40, 18.18s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 258/520 [1:15:40<1:22:44, 18.95s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 259/520 [1:15:58<1:20:56, 18.61s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 260/520 [1:16:23<1:28:43, 20.47s/it]                                                     {'loss': 0.6023, 'learning_rate': 2.5755075695022224e-05, 'epoch': 9.81}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 260/520 [1:16:23<1:28:43, 20.47s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 261/520 [1:16:40<1:24:50, 19.65s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 262/520 [1:16:58<1:22:15, 19.13s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 263/520 [1:17:12<1:15:12, 17.56s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 264/520 [1:17:27<1:11:56, 16.86s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 265/520 [1:17:43<1:09:44, 16.41s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 266/520 [1:17:58<1:07:17, 15.89s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 267/520 [1:18:17<1:10:57, 16.83s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 268/520 [1:18:33<1:10:33, 16.80s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 269/520 [1:18:48<1:07:07, 16.04s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 270/520 [1:19:08<1:11:55, 17.26s/it]                                                     {'loss': 0.6222, 'learning_rate': 2.4244924304977785e-05, 'epoch': 10.19}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 270/520 [1:19:08<1:11:55, 17.26s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 271/520 [1:19:31<1:19:47, 19.23s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 272/520 [1:19:54<1:23:28, 20.20s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 273/520 [1:20:10<1:18:40, 19.11s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 274/520 [1:20:29<1:17:40, 18.94s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 275/520 [1:20:49<1:18:04, 19.12s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 276/520 [1:21:02<1:11:18, 17.53s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 277/520 [1:21:20<1:11:03, 17.54s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 278/520 [1:21:37<1:10:21, 17.44s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 279/520 [1:21:54<1:09:39, 17.34s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 280/520 [1:22:12<1:10:07, 17.53s/it]                                                     {'loss': 0.5477, 'learning_rate': 2.2737528104259056e-05, 'epoch': 10.57}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 280/520 [1:22:12<1:10:07, 17.53s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 281/520 [1:22:30<1:09:44, 17.51s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 282/520 [1:22:44<1:05:07, 16.42s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 283/520 [1:22:54<57:22, 14.53s/it]   55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 284/520 [1:23:12<1:01:36, 15.67s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 285/520 [1:23:27<1:00:11, 15.37s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 286/520 [1:23:45<1:03:30, 16.28s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 287/520 [1:24:01<1:02:27, 16.08s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 288/520 [1:24:20<1:05:31, 16.95s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 289/520 [1:24:40<1:09:38, 18.09s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 290/520 [1:24:59<1:09:20, 18.09s/it]                                                     {'loss': 0.564, 'learning_rate': 2.1238387418130422e-05, 'epoch': 10.94}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 290/520 [1:24:59<1:09:20, 18.09s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 291/520 [1:25:16<1:08:11, 17.87s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 292/520 [1:25:35<1:09:13, 18.22s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 293/520 [1:25:54<1:09:33, 18.39s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 294/520 [1:26:09<1:05:39, 17.43s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 295/520 [1:26:27<1:06:40, 17.78s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 296/520 [1:26:48<1:09:51, 18.71s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 297/520 [1:27:08<1:11:01, 19.11s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 298/520 [1:27:28<1:11:35, 19.35s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 299/520 [1:27:49<1:12:28, 19.68s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 300/520 [1:28:06<1:09:04, 18.84s/it]                                                     {'loss': 0.475, 'learning_rate': 1.9752972448378814e-05, 'epoch': 11.32}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 300/520 [1:28:06<1:09:04, 18.84s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 301/520 [1:28:28<1:12:54, 19.97s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 302/520 [1:28:47<1:11:33, 19.70s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 303/520 [1:29:03<1:06:52, 18.49s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 304/520 [1:29:17<1:01:26, 17.07s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 305/520 [1:29:33<1:00:49, 16.97s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 306/520 [1:29:48<57:50, 16.22s/it]   59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 307/520 [1:30:05<58:01, 16.34s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 308/520 [1:30:24<1:01:15, 17.34s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 309/520 [1:30:43<1:02:20, 17.73s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 310/520 [1:30:59<1:00:51, 17.39s/it]                                                     {'loss': 0.5064, 'learning_rate': 1.828670331314058e-05, 'epoch': 11.7}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 310/520 [1:30:59<1:00:51, 17.39s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 311/520 [1:31:16<59:27, 17.07s/it]   60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 312/520 [1:31:28<54:30, 15.72s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 313/520 [1:31:44<53:52, 15.62s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 314/520 [1:31:58<52:06, 15.18s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 315/520 [1:32:17<55:21, 16.20s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 316/520 [1:32:33<55:45, 16.40s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 317/520 [1:32:49<54:36, 16.14s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 318/520 [1:33:10<59:15, 17.60s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 319/520 [1:33:26<57:16, 17.10s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 320/520 [1:33:45<59:11, 17.76s/it]                                                   {'loss': 0.5072, 'learning_rate': 1.6844930269478274e-05, 'epoch': 12.08}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 320/520 [1:33:45<59:11, 17.76s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 321/520 [1:34:04<59:34, 17.96s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 322/520 [1:34:26<1:03:15, 19.17s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 323/520 [1:34:45<1:03:32, 19.35s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 324/520 [1:35:03<1:02:00, 18.98s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 325/520 [1:35:23<1:01:46, 19.01s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 326/520 [1:35:40<1:00:18, 18.65s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 327/520 [1:36:04<1:04:21, 20.01s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 328/520 [1:36:24<1:04:15, 20.08s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 329/520 [1:36:40<1:00:14, 18.92s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 330/520 [1:36:59<59:46, 18.87s/it]                                                     {'loss': 0.4589, 'learning_rate': 1.5432914190872757e-05, 'epoch': 12.45}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 330/520 [1:36:59<59:46, 18.87s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 331/520 [1:37:16<57:48, 18.35s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 332/520 [1:37:35<58:30, 18.67s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 333/520 [1:37:55<58:57, 18.91s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 334/520 [1:38:11<55:46, 17.99s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 335/520 [1:38:26<52:54, 17.16s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 336/520 [1:38:39<49:16, 16.07s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 337/520 [1:38:58<51:31, 16.89s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 338/520 [1:39:13<49:28, 16.31s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 339/520 [1:39:30<50:00, 16.58s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 340/520 [1:39:47<49:26, 16.48s/it]                                                   {'loss': 0.4524, 'learning_rate': 1.4055807370866485e-05, 'epoch': 12.83}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 340/520 [1:39:47<49:26, 16.48s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 341/520 [1:40:07<52:50, 17.71s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 342/520 [1:40:21<49:16, 16.61s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 343/520 [1:40:39<50:16, 17.04s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 344/520 [1:40:57<50:29, 17.21s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 345/520 [1:41:16<51:50, 17.78s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 346/520 [1:41:33<50:47, 17.52s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 347/520 [1:41:51<50:48, 17.62s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 348/520 [1:42:09<51:10, 17.85s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 349/520 [1:42:27<50:54, 17.86s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 350/520 [1:42:48<53:04, 18.73s/it]                                                   {'loss': 0.4099, 'learning_rate': 1.2718634722903073e-05, 'epoch': 13.21}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 350/520 [1:42:48<53:04, 18.73s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 351/520 [1:43:05<51:16, 18.20s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 352/520 [1:43:25<52:40, 18.81s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 353/520 [1:43:42<50:50, 18.27s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 354/520 [1:43:58<48:20, 17.47s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 355/520 [1:44:15<48:16, 17.56s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 356/520 [1:44:32<47:34, 17.40s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 357/520 [1:44:49<46:22, 17.07s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 358/520 [1:45:08<47:37, 17.64s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 359/520 [1:45:27<48:24, 18.04s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 360/520 [1:45:42<45:40, 17.13s/it]                                                   {'loss': 0.3833, 'learning_rate': 1.1426275444963034e-05, 'epoch': 13.58}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 360/520 [1:45:42<45:40, 17.13s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 361/520 [1:45:57<43:48, 16.53s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 362/520 [1:46:14<44:07, 16.76s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 363/520 [1:46:29<42:05, 16.09s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 364/520 [1:46:49<44:53, 17.27s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 365/520 [1:47:04<43:21, 16.78s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 366/520 [1:47:22<43:45, 17.05s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 367/520 [1:47:40<44:30, 17.45s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 368/520 [1:47:56<43:04, 17.01s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 369/520 [1:48:13<42:25, 16.85s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 370/520 [1:48:32<43:48, 17.53s/it]                                                   {'loss': 0.3893, 'learning_rate': 1.0183445215899584e-05, 'epoch': 13.96}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 370/520 [1:48:32<43:48, 17.53s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 371/520 [1:48:48<42:27, 17.10s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 372/520 [1:49:03<40:55, 16.59s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 373/520 [1:49:19<39:44, 16.22s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 374/520 [1:49:34<39:03, 16.05s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 375/520 [1:49:57<43:39, 18.06s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 376/520 [1:50:14<42:45, 17.82s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 377/520 [1:50:31<41:22, 17.36s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 378/520 [1:50:51<43:04, 18.20s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 379/520 [1:51:07<41:35, 17.70s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 380/520 [1:51:21<38:29, 16.49s/it]                                                   {'loss': 0.3558, 'learning_rate': 8.994678988437802e-06, 'epoch': 14.34}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 380/520 [1:51:21<38:29, 16.49s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 381/520 [1:51:40<39:39, 17.12s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 382/520 [1:51:56<38:54, 16.92s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 383/520 [1:52:12<38:09, 16.71s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 384/520 [1:52:31<39:33, 17.45s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 385/520 [1:52:52<41:01, 18.24s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 386/520 [1:53:08<39:20, 17.61s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 387/520 [1:53:23<37:15, 16.80s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 388/520 [1:53:39<36:52, 16.76s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 389/520 [1:53:56<36:18, 16.63s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 390/520 [1:54:18<39:42, 18.33s/it]                                                   {'loss': 0.3946, 'learning_rate': 7.864314441624004e-06, 'epoch': 14.72}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 390/520 [1:54:18<39:42, 18.33s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 391/520 [1:54:35<38:22, 17.85s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 392/520 [1:54:52<37:28, 17.56s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 393/520 [1:55:10<37:36, 17.77s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 394/520 [1:55:28<37:38, 17.93s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 395/520 [1:55:46<37:17, 17.90s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 396/520 [1:56:05<37:28, 18.14s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 397/520 [1:56:23<37:04, 18.08s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 398/520 [1:56:40<36:09, 17.78s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 399/520 [1:56:54<33:53, 16.81s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 400/520 [1:57:08<31:54, 15.95s/it]                                                   {'loss': 0.3349, 'learning_rate': 6.796476153105294e-06, 'epoch': 15.09}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 400/520 [1:57:08<31:54, 15.95s/it][INFO|trainer.py:2841] 2023-09-28 17:27:24,562 >> Saving model checkpoint to output/dialog_inpainting_Baichuan2_chatGPT_2/checkpoint-400
[INFO|tokenization_utils_base.py:2235] 2023-09-28 17:27:24,609 >> tokenizer config file saved in output/dialog_inpainting_Baichuan2_chatGPT_2/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2242] 2023-09-28 17:27:24,609 >> Special tokens file saved in output/dialog_inpainting_Baichuan2_chatGPT_2/checkpoint-400/special_tokens_map.json
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 401/520 [1:57:26<32:41, 16.48s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 402/520 [1:57:40<30:47, 15.65s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 403/520 [1:57:57<31:36, 16.21s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 404/520 [1:58:17<33:44, 17.45s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 405/520 [1:58:37<34:39, 18.08s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 406/520 [1:58:53<33:02, 17.39s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 407/520 [1:59:12<33:38, 17.87s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 408/520 [1:59:29<33:04, 17.72s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 409/520 [1:59:45<31:59, 17.29s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 410/520 [2:00:00<30:09, 16.45s/it]                                                   {'loss': 0.335, 'learning_rate': 5.79506054899299e-06, 'epoch': 15.47}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 410/520 [2:00:00<30:09, 16.45s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 411/520 [2:00:22<32:50, 18.08s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 412/520 [2:00:44<34:39, 19.25s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 413/520 [2:01:03<34:30, 19.35s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 414/520 [2:01:21<33:14, 18.82s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 415/520 [2:01:40<33:01, 18.87s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 416/520 [2:01:58<32:33, 18.79s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 417/520 [2:02:19<33:16, 19.39s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 418/520 [2:02:35<31:11, 18.35s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 419/520 [2:02:50<29:15, 17.38s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 420/520 [2:03:12<31:05, 18.66s/it]                                                   {'loss': 0.3272, 'learning_rate': 4.86372168622635e-06, 'epoch': 15.85}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 420/520 [2:03:12<31:05, 18.66s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 421/520 [2:03:27<29:05, 17.63s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 422/520 [2:03:41<26:49, 16.42s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 423/520 [2:04:02<28:44, 17.78s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 424/520 [2:04:15<26:28, 16.55s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 425/520 [2:04:33<26:34, 16.79s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 426/520 [2:04:46<24:34, 15.68s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 427/520 [2:05:05<26:04, 16.82s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 428/520 [2:05:17<23:26, 15.29s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 429/520 [2:05:37<25:10, 16.60s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 430/520 [2:05:53<24:38, 16.43s/it]                                                   {'loss': 0.3368, 'learning_rate': 4.0058579193150535e-06, 'epoch': 16.23}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 430/520 [2:05:53<24:38, 16.43s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 431/520 [2:06:12<25:50, 17.42s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 432/520 [2:06:35<27:41, 18.88s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 433/520 [2:06:52<26:32, 18.31s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 434/520 [2:07:13<27:30, 19.19s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 435/520 [2:07:32<26:59, 19.06s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 436/520 [2:07:50<26:24, 18.86s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 437/520 [2:08:05<24:17, 17.56s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 438/520 [2:08:19<22:43, 16.62s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 439/520 [2:08:45<26:16, 19.47s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 440/520 [2:09:03<25:07, 18.85s/it]                                                   {'loss': 0.3251, 'learning_rate': 3.2245995001121106e-06, 'epoch': 16.6}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 440/520 [2:09:03<25:07, 18.85s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 441/520 [2:09:20<24:24, 18.54s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 442/520 [2:09:34<22:15, 17.12s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 443/520 [2:09:49<21:14, 16.56s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 444/520 [2:10:07<21:14, 16.77s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 445/520 [2:10:28<22:29, 17.99s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 446/520 [2:10:45<21:49, 17.70s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 447/520 [2:10:59<20:19, 16.71s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 448/520 [2:11:19<21:16, 17.73s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 449/520 [2:11:42<22:41, 19.18s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 450/520 [2:11:59<21:49, 18.70s/it]                                                   {'loss': 0.3018, 'learning_rate': 2.5227971558643537e-06, 'epoch': 16.98}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 450/520 [2:11:59<21:49, 18.70s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 451/520 [2:12:13<19:48, 17.23s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 452/520 [2:12:30<19:34, 17.27s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 453/520 [2:12:45<18:28, 16.55s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 454/520 [2:13:04<19:01, 17.30s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 455/520 [2:13:21<18:40, 17.24s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 456/520 [2:13:42<19:23, 18.18s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 457/520 [2:13:58<18:25, 17.54s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 458/520 [2:14:14<17:44, 17.16s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 459/520 [2:14:30<17:01, 16.74s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 460/520 [2:14:46<16:36, 16.61s/it]                                                   {'loss': 0.302, 'learning_rate': 1.9030116872178316e-06, 'epoch': 17.36}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 460/520 [2:14:46<16:36, 16.61s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 461/520 [2:15:02<16:11, 16.46s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 462/520 [2:15:15<14:49, 15.33s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 463/520 [2:15:36<16:12, 17.06s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 464/520 [2:15:53<15:49, 16.96s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 465/520 [2:16:09<15:20, 16.74s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 466/520 [2:16:27<15:22, 17.09s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 467/520 [2:16:42<14:34, 16.50s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 468/520 [2:17:04<15:40, 18.09s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 469/520 [2:17:22<15:21, 18.07s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 470/520 [2:17:38<14:27, 17.35s/it]                                                   {'loss': 0.2885, 'learning_rate': 1.3675046241339918e-06, 'epoch': 17.74}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 470/520 [2:17:38<14:27, 17.35s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 471/520 [2:17:56<14:30, 17.76s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 472/520 [2:18:14<14:10, 17.73s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 473/520 [2:18:32<14:00, 17.88s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 474/520 [2:18:47<13:04, 17.05s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 475/520 [2:19:04<12:44, 16.98s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 476/520 [2:19:24<13:04, 17.83s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 477/520 [2:19:39<12:08, 16.95s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 478/520 [2:19:54<11:28, 16.39s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 479/520 [2:20:12<11:33, 16.91s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 480/520 [2:20:29<11:22, 17.06s/it]                                                   {'loss': 0.3244, 'learning_rate': 9.182299738120931e-07, 'epoch': 18.11}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 480/520 [2:20:29<11:22, 17.06s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 481/520 [2:20:51<11:54, 18.32s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 482/520 [2:21:09<11:41, 18.46s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 483/520 [2:21:28<11:25, 18.52s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 484/520 [2:21:44<10:36, 17.68s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 485/520 [2:21:59<09:55, 17.01s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 486/520 [2:22:16<09:30, 16.77s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 487/520 [2:22:31<08:59, 16.34s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 488/520 [2:22:47<08:44, 16.40s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 489/520 [2:23:02<08:16, 16.01s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 490/520 [2:23:20<08:14, 16.47s/it]                                                   {'loss': 0.3053, 'learning_rate': 5.568270907288287e-07, 'epoch': 18.49}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 490/520 [2:23:20<08:14, 16.47s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 491/520 [2:23:37<08:01, 16.61s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 492/520 [2:23:52<07:29, 16.07s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 493/520 [2:24:12<07:45, 17.23s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 494/520 [2:24:33<07:56, 18.34s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 495/520 [2:24:48<07:15, 17.41s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 496/520 [2:25:10<07:29, 18.73s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 497/520 [2:25:29<07:17, 19.01s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 498/520 [2:25:47<06:51, 18.70s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 499/520 [2:26:05<06:27, 18.44s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 500/520 [2:26:24<06:08, 18.45s/it]                                                   {'loss': 0.2893, 'learning_rate': 2.846146948116468e-07, 'epoch': 18.87}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 500/520 [2:26:24<06:08, 18.45s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 501/520 [2:26:41<05:44, 18.12s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 502/520 [2:26:58<05:21, 17.85s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 503/520 [2:27:20<05:22, 18.95s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 504/520 [2:27:37<04:55, 18.44s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 505/520 [2:27:53<04:24, 17.62s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 506/520 [2:28:09<04:00, 17.19s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 507/520 [2:28:22<03:28, 16.01s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 508/520 [2:28:37<03:09, 15.79s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 509/520 [2:28:51<02:47, 15.21s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 510/520 [2:29:12<02:48, 16.82s/it]                                                   {'loss': 0.3033, 'learning_rate': 1.0258605957272627e-07, 'epoch': 19.25}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 510/520 [2:29:12<02:48, 16.82s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 511/520 [2:29:32<02:39, 17.73s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 512/520 [2:29:48<02:17, 17.16s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 513/520 [2:30:06<02:03, 17.62s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 514/520 [2:30:24<01:45, 17.56s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 515/520 [2:30:39<01:24, 16.88s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 516/520 [2:30:59<01:11, 17.79s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 517/520 [2:31:19<00:55, 18.65s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 518/520 [2:31:36<00:35, 17.87s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 519/520 [2:31:56<00:18, 18.54s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [2:32:11<00:00, 17.73s/it]                                                   {'loss': 0.2935, 'learning_rate': 1.1405387761664887e-08, 'epoch': 19.62}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [2:32:11<00:00, 17.73s/it][INFO|trainer.py:1960] 2023-09-28 18:02:27,905 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                   {'train_runtime': 9133.4461, 'train_samples_per_second': 2.768, 'train_steps_per_second': 0.057, 'train_loss': 0.7465999213548807, 'epoch': 19.62}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [2:32:11<00:00, 17.73s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 520/520 [2:32:11<00:00, 17.56s/it]
***** train metrics *****
  epoch                    =      19.62
  train_loss               =     0.7466
  train_runtime            = 2:32:13.44
  train_samples_per_second =      2.768
  train_steps_per_second   =      0.057
[INFO|trainer.py:2841] 2023-09-28 18:02:27,909 >> Saving model checkpoint to output/dialog_inpainting_Baichuan2_chatGPT_2
[INFO|tokenization_utils_base.py:2235] 2023-09-28 18:02:27,959 >> tokenizer config file saved in output/dialog_inpainting_Baichuan2_chatGPT_2/tokenizer_config.json
[INFO|tokenization_utils_base.py:2242] 2023-09-28 18:02:27,959 >> Special tokens file saved in output/dialog_inpainting_Baichuan2_chatGPT_2/special_tokens_map.json
Figure saved: output/dialog_inpainting_Baichuan2_chatGPT_2/training_loss.png
09/28/2023 18:02:28 - WARNING - llmtuner.extras.ploting - No metric eval_loss to plot.
